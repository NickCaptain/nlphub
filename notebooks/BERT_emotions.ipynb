{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    ValueError('Switch to GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/nikitakapitan/nlphub.git\n",
    "!pip install -q datasets transformers\n",
    "!pip install -q umap-learn\n",
    "!pip install -q ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.13 ms (started: 2023-01-24 18:51:28 +01:00)\n"
     ]
    }
   ],
   "source": [
    "import os   # check/load files\n",
    "import json # pprint dict\n",
    "\n",
    "import sklearn\n",
    "import sklearn.dummy\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import transformers # Hugging Face transformers\n",
    "\n",
    "from nlphub import vizual\n",
    "from nlphub import hidden_state\n",
    "from nlphub import metrics\n",
    "from nlphub import errors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%load_ext autotime\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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\n",
    "print(\"Login to Hugging Face Hub:\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIZUAL = True\n",
    "HIDDEN_STATE_AS_FEATURES = True\n",
    "ERROR_ANALYSIS = True\n",
    "CHECKPOINT = 'distilbert-base-uncased'\n",
    "\n",
    "\n",
    "dataset = datasets.load_dataset('emotion')\n",
    "print('Step 1. Load DATA :', dataset['train'].builder_name)\n",
    "\n",
    "VIZUAL and vizual.output_distribution(dataset=dataset)\n",
    "\n",
    "\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained(CHECKPOINT)\n",
    "tokenize = lambda batch : tokenizer(batch['text'], padding=True)\n",
    "print('Step 2. Loaded TOKENIZER :', type(tokenizer))\n",
    "\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "print('OK. DATA is encoded by TOKENIZER')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('OK. Loaded DEVICE :', device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 4 min GPU\n",
    "print('Optional 1 : HIDDEN_STATE as Features analysis...')\n",
    "if HIDDEN_STATE_AS_FEATURES:\n",
    "    \n",
    "    model = transformers.DistilBertModel.from_pretrained(CHECKPOINT)\n",
    "    model.to(device)\n",
    "    print('... optional 1. Loaded MODEL for feature extraction :', type(model))\n",
    "    \n",
    "    if os.path.exists('emos_hidden'):\n",
    "        dataset_hidden = datasets.load_from_disk('emos_hidden') \n",
    "    else:\n",
    "        print(f'... optinal 1. Getting Hidden_state for {len(dataset_encoded[\"train\"])} examples ~ 3 min')\n",
    "        dataset_hidden = hidden_state.get_hidden_state(data_encoded=dataset_encoded, model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    labels = dataset_hidden[\"train\"].features[\"label\"].names\n",
    "    X_train, X_valid, y_train, y_valid = hidden_state.prepare_data(data_hidden=dataset_hidden)\n",
    "\n",
    "    # UMAP2D projection\n",
    "    VIZUAL and vizual.plot_umap(X_train=X_train, y_train=y_train, labels=labels)\n",
    "\n",
    "    # Dummy [Most Frequent] Classification\n",
    "    dummy_clf = sklearn.dummy.DummyClassifier(strategy='most_frequent')\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "    print('... optional 1. Dummy [Most Frequent] Classifier score:', dummy_clf.score(X_valid, y_valid))\n",
    "\n",
    "    print('... optional 1. Making LogisticRegression Classification (GPU: ~2 min / CPU: ~20 min)')\n",
    "    lr_clf = sklearn.linear_model.LogisticRegression(max_iter=3000)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('... optional 1. LogReg trained on last hidden state score:', lr_clf.score(X_valid, y_valid))\n",
    "\n",
    "    VIZUAL and vizual.plot_confusion_matrix(y_preds=lr_clf.predict(X_valid), y_true=y_valid, labels=labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*★*:;;;;;:*★*:;;;;;:*★* FINE-TUNNING BERT *★*:;;;;;:*★*:;;;;;:*★*')\n",
    "\n",
    "num_labels = 6\n",
    "# AutoModel4SeqClass adds untrained head for classification on top of ref_model feature extractors. use for init ONLY (not import)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(CHECKPOINT, num_labels=num_labels)\n",
    "model = model.to(device)\n",
    "print('Step 3. Loaded MODEL for classification:', type(model))\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(dataset_encoded['train']) // batch_size\n",
    "model_name = f'{CHECKPOINT}-finetuned-{dataset[\"train\"].builder_name}'\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch', # evaluation at the end of each epoch\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    "    log_level=\"error\",\n",
    "    )\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    compute_metrics = metrics.compute_metrics,\n",
    "    train_dataset = dataset_encoded['train'],\n",
    "    eval_dataset = dataset_encoded['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 4 min GPU\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(preds_output) = PredictionOutput\n",
    "preds_output = trainer.predict(dataset_encoded['validation'])\n",
    "print('... Result 3 : validation metrics', json.dumps(preds_output.metrics, indent = 4))\n",
    "\n",
    "# greedy predictions\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "\n",
    "VIZUAL and vizual.plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*★*:;;;;;:*★*:;;;;;:*★* ERROR ANALYSIS *★*:;;;;;:*★*:;;;;;:*★*')\n",
    "\n",
    "\"\"\"\"\n",
    "Recall: \n",
    "if model.config.problem_type == \"regression\":\n",
    "    loss_fct = MSELoss()\n",
    "elif model.config.problem_type == \"single_label_classification\":\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "elif model.config.problem_type == \"multi_label_classification\":\n",
    "    loss_fct = BCEWithLogitsLoss()\n",
    "\"\"\"\n",
    "df = None\n",
    "if ERROR_ANALYSIS:\n",
    "    df = errors.error_analysis(dataset_encoded, model=model, device=device, tokenizer=tokenizer)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SAVING MODEL TO HUB')\n",
    "trainer.push_to_hub(commit_message='Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load model and Make predictions')\n",
    "\n",
    "custom_tweet = \"I saw a movie today and it was really good.\"\n",
    "\n",
    "model_id = \"nikitakapitan/distilbert-base-uncased-finetuned-emotion\"\n",
    "\n",
    "classifier = transformers.pipeline(\"text-classification\", model=model_id)\n",
    "preds = classifier(custom_tweet, return_all_scores=True)\n",
    "\n",
    "VIZUAL and vizual.plt_bar(preds, labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0159b81555e194df7357f3ec66ac7a725116f7bdd06d05856166e90d27da3b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
