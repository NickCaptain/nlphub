{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan : NER with XLM-RoBERTa\n",
    "\n",
    "Task: **perform NER for a Switzerland user** \n",
    "\n",
    "input in Switzerland = 63% DE + 23% FR + 8% IT + 6% EN\n",
    "\n",
    "We will:\n",
    "- import XLM-RoBERTa (Cross-Language Model based on RoBERTa)\n",
    "- add token-classification head for Named Entity Recognition(NER) \n",
    "- Fine-tune multi-ling PANX dataset \n",
    "\n",
    "\n",
    "### Notebook step by step:\n",
    "1. Load 4 datasets: load_dataset('xtreme', name=lang), lang in [de, fr, it, en]\n",
    "2. create custom myXLMRoBERTaForTokenClassification\n",
    "3. Prepare Batches of Training Data : tokenize, tag as IGN subseq-subwords\n",
    "4. Fine tune XML-RoBERTa to PANX.de (aka train)\n",
    "5. Cross-Lingual Transfer: use DE-fine-tuned model to zero-shot on FR,IT and ENG\n",
    "6. Analyze DE zero-shot: FR-fine-tune f1 score as func of train_examples\n",
    "7. Fine-tunning on Multiple Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging # hide logs like cache import\n",
    "\n",
    "logging.getLogger('datasets.builder').setLevel(logging.ERROR)\n",
    "logging.getLogger('datasets.arrow_dataset').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    ValueError(\"Switch to GPU!\")\n",
    "\n",
    "# Install packages\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    packages = [\"transformers\", \"datasets\", \"accelerate\", \"pynvml\", \"seqeval\"]\n",
    "\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pynvml import *\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "DUMMY = False\n",
    "WORKMODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset : XTREME\n",
    "\n",
    "WikiANN : Wikipedia articles in many languages.\n",
    "Each article is annotated with LOC, PER, ORG in IOB2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6631d8a4b14a5fad3a0d7713f38400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e03f2429ecd410480a9f443fe6815ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd3782c083e4fef8751592a5a8c0552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f08a12d0da4ef2bd4127d06cbe25a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 228 MB.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "langs = [\"de\", \"fr\", \"it\", \"en\"] # CH languages\n",
    "fracs = [0.629, 0.229, 0.084, 0.059] # CH spoken ration\n",
    "panx_ch = defaultdict(DatasetDict) # return DatasetDict if key doesnt exist\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    \n",
    "    # DatasetDict. keys={'train', 'validation', 'test'}, values = Dataset\n",
    "    monolingual_ds = load_dataset(\"xtreme\", name=f'PAN-X.{lang}')\n",
    "\n",
    "    for split in monolingual_ds:\n",
    "        # shuffle to avoid bias & downsample according to spoken ratio\n",
    "        panx_ch[lang][split] = (\n",
    "            monolingual_ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * monolingual_ds[split].num_rows))))\n",
    "        \n",
    "print(print_gpu_utilization())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tokens', ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.'])\n",
      "('ner_tags', [0, 0, 0, 0, 0, 5, 6, 6, 0])\n",
      "('langs', ['fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**</td>\n",
       "      <td>À</td>\n",
       "      <td>l'ouest</td>\n",
       "      <td>par</td>\n",
       "      <td>la</td>\n",
       "      <td>province</td>\n",
       "      <td>de</td>\n",
       "      <td>Banten</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1        2    3   4         5      6       7  8\n",
       "0  **  À  l'ouest  par  la  province     de  Banten  .\n",
       "1   O  O        O    O   O     B-LOC  I-LOC   I-LOC  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in panx_ch['fr']['train'][200].items():\n",
    "    print(item)\n",
    "\n",
    "# ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "\n",
    "def create_tag_names(batch):\n",
    "    return {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "\n",
    "panx_fr = panx_ch['fr'].map(create_tag_names)\n",
    "fr_example = panx_fr['train'][200]\n",
    "pd.DataFrame([fr_example['tokens'], fr_example['ner_tags_str']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Custom RoBERTa-based Model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class MyXLMRoBERTaForTokenClassification(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # init RoBERTa with config (class XLMRobertaConfig)\n",
    "        self.num_labels =  config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # build classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # load body pre-trained weights (RoBERTa) & init classifier weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "\n",
    "        encode = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        \n",
    "        # classification\n",
    "        output = self.dropout(encode[0]) # [CLS]\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # logits (B, N, K)\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states = encode.hidden_states, attentions=encode.attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "\n",
    "index2tag = {idx : tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag : idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "# init xlmr tokenizer\n",
    "xlmr_tokenizer = transformers.AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "\n",
    "# we dont AutoModel.from_pretrained(xmlr_model_name) \n",
    "# because we need to overwrite number of output classes and mappings\n",
    "# thus, we AutoConfig.from_pretrained(..) first then AutoModel.from_pretrained(NewConfig)\n",
    "xmlr_config = transformers.AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                                      num_labels=tags.num_classes,\n",
    "                                                      id2label=index2tag,\n",
    "                                                      label2id=tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's init dummy model and make dummy inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # text = \"Jack Sparrow ..\"\n",
    "\n",
    "    # {'input_ids' : [0, 217, 37 .. 2], 'attention_mask' : [1,1...1]}\n",
    "    tokenizer_output = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # [<s>\t▁Jack\t▁Spar\trow\t...\t</s>]\n",
    "    tokens = tokenizer_output.tokens()\n",
    "\n",
    "    # [0, 217, 37 .. 2]|\n",
    "    input_ids = tokenizer_output.input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # [ [0.3, 0.1, 0.6] , [0.4, 0.9, 0.2] , ... [-0.5, 0.1, -0.3] ]\n",
    "    outputs = model(input_ids)[0] # <- (logits, hidden_state)[0]\n",
    "    print(f'{outputs.shape=}')\n",
    "\n",
    "    # [0, 2, 3, 3, 0, 0, 5, 6, 6, 0, 0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    # [O, B-PER, I-PER, I-PER, O, O, B-LOC, I-LOC, O, O]\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    print_gpu_utilization()\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "if DUMMY:\n",
    "    # MyXLMRoBERTaForTokenClassification.from_pretrained(..) is inherited from RobertaPreTrainedModel\n",
    "    xlmr_model = MyXLMRoBERTaForTokenClassification.from_pretrained(\n",
    "        xlmr_model_name, config=xmlr_config).to(device)\n",
    "\n",
    "    text = \"Jack Sparrow loves New Yourk!\"\n",
    "    tag_text(text, tags, xlmr_model, xlmr_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Before we prepare train Batch, let's check how tokenizer tokenize\n",
    "- Problem: tokenizer splits ONE \"l'ouest\" -> FOUR tokens '▁l', \"'\", 'ou', 'est'\n",
    "- Solution: thus we don't need to NER 3 subwords and need to mask them for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words) = 9\n",
      "words=['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
      "labels=[0, 0, 0, 0, 0, 5, 6, 6, 0]\n",
      "\n",
      "len(tokens) = 16\n",
      "tokens=['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁**</td>\n",
       "      <td>▁À</td>\n",
       "      <td>▁l</td>\n",
       "      <td>'</td>\n",
       "      <td>ou</td>\n",
       "      <td>est</td>\n",
       "      <td>▁par</td>\n",
       "      <td>▁la</td>\n",
       "      <td>▁province</td>\n",
       "      <td>▁de</td>\n",
       "      <td>▁Ban</td>\n",
       "      <td>ten</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1   2   3    4    5    6     7    8          9      10     11   12  \\\n",
       "0  <s>  ▁**  ▁À  ▁l    '   ou  est  ▁par  ▁la  ▁province    ▁de   ▁Ban  ten   \n",
       "1  IGN    O   O   O  IGN  IGN  IGN     O    O      B-LOC  I-LOC  I-LOC  IGN   \n",
       "\n",
       "  13   14    15  \n",
       "0  ▁    .  </s>  \n",
       "1  O  IGN   IGN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not WORKMODE: \n",
    "    ####################################### Tokenize example #######################################\n",
    "    words, labels = fr_example['tokens'], fr_example['ner_tags']\n",
    "    print(f'len(words) = {len(words)}\\n{words=}')\n",
    "    print(f'{labels=}')\n",
    "\n",
    "    # {input_ids : [0, 96, 25, 796, 525 ... ], attention_mask : [1 1 .. 1] }\n",
    "    tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)\n",
    "\n",
    "    # ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est' ... ]\n",
    "    tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input.input_ids)\n",
    "    print(f'\\nlen(tokens) = {len(tokens)}\\n{tokens=}')\n",
    "\n",
    "    ########################## MASK subsequent subwords by setting label = -100 ##########################\n",
    "\n",
    "    # [None 0, 1, 2, 2, 2, 2, 3, 4, 5, 6, 7, 7, 8, 8, None]\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    # set -100 for subseq-subwords or None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            label_ids.append(-100) # nn.CrossEntropyLoss ignore_index == -100\n",
    "        else:\n",
    "            label_ids.append(labels[word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    # overwrite labels with IGN subwords\n",
    "    labels = [index2tag[l] if l!= -100 else \"IGN\" for l in label_ids]\n",
    "\n",
    "    pd.DataFrame([tokens, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Now, let's prepare Train Batch\n",
    "We define single fucntion to wrap the previous logic for all Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\" examples - Dataset (set of all examples)\n",
    "    # examples[0] - dict | keys='tokens' 'ner_tags' 'langs' 'ner_tags_str'\n",
    "    \"\"\"\n",
    "\n",
    "    # {input_ids : [[0, 96, ..], [0, 25 ..]] attention_mask: [[1..1], [1..1]]}\n",
    "    tokenized_inputs = xlmr_tokenizer(examples['tokens'],\n",
    "                                      truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    new_labels = []\n",
    "    for idx, label in enumerate(examples['ner_tags']):\n",
    "\n",
    "        # [None 0, 1, 1, 2, 3, 3, 3, 4, 5, None]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # set -100 for None or subseq-subwords\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # [ [-100, 5, -100, -100, 6, 6, -100 ..], ...]\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "panx_encoded = {}\n",
    "\n",
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])\n",
    "\n",
    "panx_encoded['fr'] = encode_panx_dataset(panx_ch['fr'])\n",
    "\n",
    "\"\"\" BEFORE:\n",
    "words=     ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
    "ner_tags:  [   0,  0,          0,     0,    0,          5,    6,        6,  0 ]\n",
    "\n",
    "AFTER:\n",
    "tokens=          ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁',  '.', '</s>']\n",
    "Encoded labels:  [-100,     0,     0,   0,  -100, -100, -100,      0,      0,          5,      6,      6,   -100,   0, -100,   -100]\n",
    "\"\"\"\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "We can measure **word-level** or whole **sequence-level** performance\n",
    "\n",
    "## Word-level performance\n",
    "\n",
    "To measure words in sequence, predictions need to be **list of lists**. Let's align predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def align_predictions(logits, label_ids):\n",
    "    # logits    (B, N, K)\n",
    "    # label_ids (B, N)\n",
    "    preds = np.argmax(logits, axis=2) # (B, N)\n",
    "    bs, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(bs):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    # preds_list = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], ... [\"B-PER\", \"I-PER\"]]\n",
    "    # labels_list = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], .. [\"B-PER\", \"I-PER\"]]\n",
    "    return preds_list, labels_list\n",
    "\n",
    "# example\n",
    "y_true = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune XMLRoBERTa\n",
    "\n",
    "As GERMAN is more representable, let's fine-tune base model on the PAN-X.de.\n",
    "\n",
    "Then we will evaluate its zero-shot cross-lingual performance on FR, IT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# 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\n",
    "# notebook_login() \n",
    "\n",
    "panx_encoded['de'] = encode_panx_dataset(panx_ch['de'])\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "log_steps = len(panx_encoded['de']['train']) // batch_size\n",
    "model_name = f'{xlmr_model_name}-finetuned-panx-de'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=log_steps,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "# tell Trainer how to compute metrics on the validation set:\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true=y_true, y_pred=y_pred)}\n",
    "\n",
    "# define data collator to PAD each input sequence to the largest sequence length\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    return (MyXLMRoBERTaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xmlr_config)\n",
    "            .to(device))\n",
    "\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_encoded['de']['train'],\n",
    "                  eval_dataset=panx_encoded['de']['validation'],\n",
    "                  tokenizer=xlmr_tokenizer\n",
    "                  )\n",
    "\n",
    "def get_f1(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORKSPACE\\nlphub\\BERT\\[NER]XLM-RoBERTa\\xlm-roberta-base-finetuned-panx-de is already a clone of https://huggingface.co/nikitakapitan/xlm-roberta-base-finetuned-panx-de. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "error: Unable to create 'D:/WORKSPACE/nlphub/BERT/[NER]XLM-RoBERTa/xlm-roberta-base-finetuned-panx-de/.git/index.lock': File exists.\n\nAnother git process seems to be running in this repository, e.g.\nan editor opened by 'git commit'. Please make sure all processes\nare terminated then try again. If it still fails, a git process\nmay have crashed in this repository earlier:\nremove the file manually to continue.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\repository.py:984\u001b[0m, in \u001b[0;36mRepository.git_pull\u001b[1;34m(self, rebase, lfs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mwith\u001b[39;00m _lfs_log_progress():\n\u001b[1;32m--> 984\u001b[0m     result \u001b[39m=\u001b[39m run_subprocess(command, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_dir)\n\u001b[0;32m    985\u001b[0m     logger\u001b[39m.\u001b[39minfo(result\u001b[39m.\u001b[39mstdout)\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_subprocess.py:83\u001b[0m, in \u001b[0;36mrun_subprocess\u001b[1;34m(command, folder, check, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m     folder \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(folder)\n\u001b[1;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m     84\u001b[0m     command,\n\u001b[0;32m     85\u001b[0m     stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[0;32m     86\u001b[0m     stdout\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[0;32m     87\u001b[0m     check\u001b[39m=\u001b[39;49mcheck,\n\u001b[0;32m     88\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     89\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreplace\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# if not utf-8, replace char by �\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m     cwd\u001b[39m=\u001b[39;49mfolder \u001b[39mor\u001b[39;49;00m os\u001b[39m.\u001b[39;49mgetcwd(),\n\u001b[0;32m     91\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     92\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[1;32m--> 571\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[0;32m    572\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[0;32m    573\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['git', 'pull']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model_init\u001b[39m=\u001b[39;49mmodel_init,\n\u001b[0;32m      4\u001b[0m                   args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[0;32m      5\u001b[0m                   data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m      6\u001b[0m                   compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[0;32m      7\u001b[0m                   train_dataset\u001b[39m=\u001b[39;49mpanx_de_encoded[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      8\u001b[0m                   eval_dataset\u001b[39m=\u001b[39;49mpanx_de_encoded[\u001b[39m'\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      9\u001b[0m                   tokenizer\u001b[39m=\u001b[39;49mxlmr_tokenizer\n\u001b[0;32m     10\u001b[0m                   )\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCOLAB_GPU\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[0;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart Training!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:563\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39m# Create clone of distant repo and output directory if needed\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub:\n\u001b[1;32m--> 563\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_git_repo(at_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    564\u001b[0m     \u001b[39m# In case of pull, we need to make sure every process has the latest.\u001b[39;00m\n\u001b[0;32m    565\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3580\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[1;34m(self, at_init)\u001b[0m\n\u001b[0;32m   3577\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3578\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m-> 3580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepo\u001b[39m.\u001b[39;49mgit_pull()\n\u001b[0;32m   3582\u001b[0m \u001b[39m# By default, ignore the checkpoint folders\u001b[39;00m\n\u001b[0;32m   3583\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   3584\u001b[0m     \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moutput_dir, \u001b[39m\"\u001b[39m\u001b[39m.gitignore\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   3585\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_strategy \u001b[39m!=\u001b[39m HubStrategy\u001b[39m.\u001b[39mALL_CHECKPOINTS\n\u001b[0;32m   3586\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\repository.py:987\u001b[0m, in \u001b[0;36mRepository.git_pull\u001b[1;34m(self, rebase, lfs)\u001b[0m\n\u001b[0;32m    985\u001b[0m         logger\u001b[39m.\u001b[39minfo(result\u001b[39m.\u001b[39mstdout)\n\u001b[0;32m    986\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m--> 987\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(exc\u001b[39m.\u001b[39mstderr)\n",
      "\u001b[1;31mOSError\u001b[0m: error: Unable to create 'D:/WORKSPACE/nlphub/BERT/[NER]XLM-RoBERTa/xlm-roberta-base-finetuned-panx-de/.git/index.lock': File exists.\n\nAnother git process seems to be running in this repository, e.g.\nan editor opened by 'git commit'. Please make sure all processes\nare terminated then try again. If it still fails, a git process\nmay have crashed in this repository earlier:\nremove the file manually to continue.\n"
     ]
    }
   ],
   "source": [
    "if not WORKMODE:\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        print('Start Training!')\n",
    "        trainer.args.push_to_hub = True\n",
    "        trainer.train() # 10 min on Colab GPU\n",
    "        trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "        trainer.save_model('colab_ner_training') # .save_model(\"/path/to/save\")\n",
    "    else:\n",
    "        print(\"Load model weights!\")\n",
    "        trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "                .from_pretrained('../../docs/colab_training/de')\n",
    "                .to(device))\n",
    "        \n",
    "    print_gpu_utilization()\n",
    "\n",
    "    text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "    tag_text(text_de, tags, trainer.model, xlmr_tokenizer)\n",
    "\n",
    "    f1 = defaultdict(dict)\n",
    "    f1[\"de\"][\"de\"] = get_f1(trainer, dataset=panx_encoded['de'][\"test\"])\n",
    "    print(f\"{f1['de']['de']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NER tags, It works!\n",
    "\n",
    "87% f1 DE test: Nice.\n",
    "\n",
    "Now let's move on to Miltu Languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross-Lingual Transfer\n",
    "\n",
    "Let's quickly shot FR on DE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\n",
    "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)\n",
    "\n",
    "text_fake = \"Nikita poshel v magazin v Amerike i uvidel druga iz Googla\"\n",
    "tag_text(text_fake, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "1. Model managed to zero-transfer LOC \"_Kali\"(DE) to \"_Cali\"(FR).\n",
    "2. zero-shot for non-existent language \"Ruskiy\" (Russian using Latin letters) also does good NER\n",
    "\n",
    "\n",
    "Let's evaluate DE-model on FR, IT and EN test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_lang = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1(trainer, dataset=panx_lang[\"test\"])\n",
    "\n",
    "f1['de']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['de']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['de']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['de']['de']=}\") # 0.867\n",
    "print(f\"{f1['de']['fr']=}\") # 0.705\n",
    "print(f\"{f1['de']['it']=}\") # 0.675\n",
    "print(f\"{f1['de']['en']=}\") # 0.586\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% and 67% good FR/IT zero-shot transfer. The model has never seen a single labeled Frech or Italian example.\n",
    "\n",
    "58% is okay, but ENG shot from DE expected to be higher.\n",
    "\n",
    "Let's now analyze how good was DE zero-shot on FR:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine-tune FR on 250-500-1k-2k-4k examples\n",
    "\n",
    "Let's fine-tune base XLMRoBERTa on FR using different len of train dataset to compare with DE zero-shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset['train'].shuffle(seed=42).select(range(num_samples))\n",
    "    valid_ds = dataset['validation']\n",
    "    test_ds = dataset['test']\n",
    "\n",
    "    training_args.push_to_hub = False\n",
    "    training_args.logging_steps = len(train_ds)\n",
    "\n",
    "    trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                      data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                      train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    return get_f1(trainer, test_ds)\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    f1s = {}\n",
    "    for ns in [250, 500, 1000, 2000, 4000]:\n",
    "        f1s[ns] = train_on_subset(dataset=panx_encoded['fr'], num_samples=ns)\n",
    "else:\n",
    "    f1s = {\"num_samples\": [250, 500, 1000, 2000, 4000, 15000], \n",
    "          \"f1\": [0.179, 0.604, 0.733, 0.817, 0.838, 0.87]} # result from colab training\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(f1[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\n",
    "metrics_df.set_index(\"num_samples\").plot(ax=ax)\n",
    "plt.legend([\"12k DE Zero-shot\", \"(x)k FR Fine-tuned\"], loc=\"lower right\")\n",
    "plt.ylim((0, 1))\n",
    "plt.title(\"F1 score on PANX.fr[test] by models\")\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"F1 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Fine-tuning on 2x Languages\n",
    "\n",
    "### First, let's concat DE and FR datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "panx_encoded['defr'] = DatasetDict()\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    panx_encoded['defr'][split] = concatenate_datasets([panx_encoded['de'][split],  panx_encoded['fr'][split]]).shuffle()\n",
    "\n",
    "\n",
    "training_args.logging_steps = len(panx_encoded['defr']['train']) // batch_size\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer, train_dataset=panx_encoded['defr'][\"train\"],\n",
    "    eval_dataset=panx_encoded['defr'][\"validation\"])\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    trainer.train()\n",
    "    trainer.save_model('defr')\n",
    "else:\n",
    "    trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "             .from_pretrained('../../docs/colab_training/defr')\n",
    "             .to(device))\n",
    "\n",
    "f1['defr']['de'] = evaluate_lang_performance('de', trainer)   \n",
    "f1['defr']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['defr']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['defr']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['defr']['de']=}\") # 0.870\n",
    "print(f\"{f1['defr']['fr']=}\") # 0.887\n",
    "print(f\"{f1['defr']['it']=}\") # 0.814\n",
    "print(f\"{f1['defr']['en']=}\") # 0.677"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
