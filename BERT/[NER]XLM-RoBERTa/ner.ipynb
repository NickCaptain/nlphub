{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan : NER with XLM-RoBERTa\n",
    "\n",
    "Task: **perform NER for a Switzerland customer**\n",
    "\n",
    "We will:\n",
    "- import XLM-RoBERTa (Cross-Language Model based on RoBERTa)\n",
    "- Fine-tune to Named Entity Recognition(NER)\n",
    "\n",
    "Switzerland = 63% DE + 23% FR + 8% IT + 6% EN\n",
    "\n",
    "As DE is most spoken, we will use DE for zero-shot cross-lingual transfer to FR, IT and EN.\n",
    "\n",
    "## Detailed plan\n",
    "1. Load 4 datasets: load_dataset('xtreme', name='fr'). + de + en + it\n",
    "    - inspect one example|\n",
    "2. create custom myXLMRoBERTaForTokenClassification\n",
    "    - define class\n",
    "    - make dummy inference\n",
    "3. Prepare Batches of Training Data\n",
    "    - tokenize\n",
    "    - tag as IGN subsequent subwords (from tokenization)\n",
    "    - remove original 'langs' 'ner_tags' 'tokens' keys from Dataset\n",
    "4. Fine tune XML-RoBERTa to PANX.de\n",
    "    - train\n",
    "5. Error Analysis\n",
    "\n",
    "### Example:\n",
    "\n",
    "['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
    "\n",
    "labels = 0    0   0\t0\t0\t  5 \t  6 \t  6 \t0\n",
    "\n",
    "tags   = O    O   O\tO\tO\tB-LOC\tI-LOC\tI-LOC\tO\n",
    "\n",
    "### Result:\n",
    "\n",
    "tokens=['< s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '< /s>']\n",
    "\n",
    "labels =  [-100, 0, 0, 0, -100, -100, -100, 0, 0, 5, 6, 6, -100, 0, -100, -100]\n",
    "\n",
    "tags   = [IGN, O, O, O, IGN, IGN, IGN, O, O, B-LOC, I-LOC, I-LOC, IGN, O, IGN, IGN]\n",
    "\n",
    "\n",
    "\n",
    "# 1. Dataset : EXTREME\n",
    "\n",
    "WikiANN : Wikipedia articles in many languages.\n",
    "Each article is annotated with LOC, PER, ORG in IOB2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    ValueError(\"Switch to GPU!\")\n",
    "\n",
    "# Install packages\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    packages = [\"transformers\", \"datasets\", \"accelerate\", \"pynvml\", \"seqeval\"]\n",
    "\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pynvml import *\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ce3898f1ef4fa0959cd416c2d48de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.de\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-e5ddf09f1ae095ec.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.de\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-25e7e2dd003d0fa6.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.de\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-73a95bc0accfea8b.arrow\n",
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3319b3ab5244fb5ba027e2a052ce5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-6ff29513007ec78b.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-c5c9a4fc19dfd7d6.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-9711ab25936b81b7.arrow\n",
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96623212bd3f46d78aabcb32b90d05aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-daa9a1770078307c.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-5e244c05031bab3c.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-497ee15c12bff58d.arrow\n",
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adee2321bcb4215a59416900740938a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-757845faa9fa6949.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-305cefc7ffa49fd9.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-e5ec5e6ba7c1237d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 135 MB.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "langs = [\"de\", \"fr\", \"it\", \"en\"] # CH languages\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "panx_ch = defaultdict(DatasetDict) # return DatasetDict if key doesnt exist\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    \n",
    "    # DatasetDict. keys={'train', 'validation', 'test'}, values = Dataset\n",
    "    monolingual_ds = load_dataset(\"xtreme\", name=f'PAN-X.{lang}')\n",
    "\n",
    "    for split in monolingual_ds:\n",
    "        # shuffle to avoid bias & downsample according to spoken ratio\n",
    "        panx_ch[lang][split] = (\n",
    "            monolingual_ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * monolingual_ds[split].num_rows))))\n",
    "        \n",
    "print(print_gpu_utilization())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Inspect one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tokens', ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.'])\n",
      "('ner_tags', [0, 0, 0, 0, 0, 5, 6, 6, 0])\n",
      "('langs', ['fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr'])\n"
     ]
    }
   ],
   "source": [
    "for item in panx_ch['fr']['train'][200].items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-8d1477a4eff16eb5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-0832c54610a26671.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-af085c7a24eed84e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tags=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**</td>\n",
       "      <td>À</td>\n",
       "      <td>l'ouest</td>\n",
       "      <td>par</td>\n",
       "      <td>la</td>\n",
       "      <td>province</td>\n",
       "      <td>de</td>\n",
       "      <td>Banten</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1        2    3   4         5      6       7  8\n",
       "0  **  À  l'ouest  par  la  province     de  Banten  .\n",
       "1   O  O        O    O   O     B-LOC  I-LOC   I-LOC  O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make tags in human-readable format\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "print(f'\\n{tags=}')\n",
    "\n",
    "def create_tag_names(batch):\n",
    "    return {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "\n",
    "panx_fr = panx_ch['fr'].map(create_tag_names)\n",
    "fr_example = panx_fr['train'][200]\n",
    "pd.DataFrame([fr_example['tokens'], fr_example['ner_tags_str']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Custom RoBERTa-based Model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class MyXLMRoBERTaForTokenClassification(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # init RoBERTa with config (class XLMRobertaConfig)\n",
    "        self.num_labels =  config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # build classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # load body pre-trained weights (RoBERTa) & init classifier weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "\n",
    "        encode = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        \n",
    "        # classification\n",
    "        output = self.dropout(encode[0]) # [CLS]\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # logits (B, N, K)\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states = encode.hidden_states, attentions=encode.attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing MyXLMRoBERTaForTokenClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyXLMRoBERTaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyXLMRoBERTaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyXLMRoBERTaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1337 MB.\n"
     ]
    }
   ],
   "source": [
    "index2tag = {idx : tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag : idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "# init xlmr tokenizer\n",
    "xlmr_tokenizer = transformers.AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "\n",
    "# we dont AutoModel.from_pretrained(xmlr_model_name) \n",
    "# because we need to overwrite number of output classes and mappings\n",
    "# thus, we AutoConfig.from_pretrained(..) first then AutoModel.from_pretrained(NewConfig)\n",
    "xmlr_config = transformers.AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                                      num_labels=tags.num_classes,\n",
    "                                                      id2label=index2tag,\n",
    "                                                      label2id=tag2index)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# init our model\n",
    "# MyXLMRoBERTaForTokenClassification.from_pretrained(..) is inherited from RobertaPreTrainedModel\n",
    "xlmr_model = MyXLMRoBERTaForTokenClassification.from_pretrained(\n",
    "    xlmr_model_name, config=xmlr_config).to(device)\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make dummy inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape=torch.Size([1, 11, 7])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁Your</td>\n",
       "      <td>k</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2    3      4  5     6      7  8  9     10\n",
       "Tokens  <s>  ▁Jack  ▁Spar  row  ▁love  s  ▁New  ▁Your  k  !  </s>\n",
       "Tags      O      O      O    O  B-ORG  O     O  B-ORG  O  O     O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Jack Sparrow loves New Yourk!\"\n",
    "\n",
    "def tag_text(text):\n",
    "    # {'input_ids' : [0, 217, 37 .. 2], 'attention_mask' : [1,1...1]}\n",
    "    tokenizer_output = xlmr_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # [<s>\t▁Jack\t▁Spar\trow\t...\t</s>]\n",
    "    tokens = tokenizer_output.tokens()\n",
    "\n",
    "    # [0, 217, 37 .. 2]\n",
    "    input_ids = tokenizer_output.input_ids\n",
    "\n",
    "    # [ [0.3, 0.1, 0.6] , [0.4, 0.9, 0.2] , ... [-0.5, 0.1, -0.3] ]\n",
    "    outputs = xlmr_model(input_ids.to(device))[0]\n",
    "    print(f'{outputs.shape=}')\n",
    "\n",
    "    # [0, 2, 3, 3, 0, 0, 5, 6, 6, 0, 0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    # [O, B-PER, I-PER, I-PER, O, O, B-LOC, I-LOC, O, O]\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "tag_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Now, let's prepare Train Batch\n",
    "\n",
    "### First, let's check how tokenizer tokenize\n",
    "- tokenizer splits ONE \"l'ouest\" -> FOUR tokens '▁l', \"'\", 'ou', 'est'\n",
    "- thus we don't need to NER 3 subwords and need to mask them for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words) = 9\n",
      "words=['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
      "labels=[0, 0, 0, 0, 0, 5, 6, 6, 0]\n",
      "\n",
      "len(tokens) = 16\n",
      "tokens=['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁**</td>\n",
       "      <td>▁À</td>\n",
       "      <td>▁l</td>\n",
       "      <td>'</td>\n",
       "      <td>ou</td>\n",
       "      <td>est</td>\n",
       "      <td>▁par</td>\n",
       "      <td>▁la</td>\n",
       "      <td>▁province</td>\n",
       "      <td>▁de</td>\n",
       "      <td>▁Ban</td>\n",
       "      <td>ten</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1   2   3    4    5    6     7    8          9      10     11   12  \\\n",
       "0  <s>  ▁**  ▁À  ▁l    '   ou  est  ▁par  ▁la  ▁province    ▁de   ▁Ban  ten   \n",
       "1  IGN    O   O   O  IGN  IGN  IGN     O    O      B-LOC  I-LOC  I-LOC  IGN   \n",
       "\n",
       "  13   14    15  \n",
       "0  ▁    .  </s>  \n",
       "1  O  IGN   IGN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, labels = fr_example['tokens'], fr_example['ner_tags']\n",
    "print(f'len(words) = {len(words)}\\n{words=}')\n",
    "print(f'{labels=}')\n",
    "\n",
    "# {input_ids : [0, 96, 25, 796, 525 ... ], attention_mask : [1 1 .. 1] }\n",
    "tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)\n",
    "\n",
    "# ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est' ... ]\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input.input_ids)\n",
    "print(f'\\nlen(tokens) = {len(tokens)}\\n{tokens=}')\n",
    "\n",
    "# MASK subsequent subwords by setting label = -100\n",
    "\n",
    "# [None 0, 1, 2, 2, 2, 2, 3, 4, 5, 6, 7, 7, 8, 8, None]\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "# set -100 for subseq-subwords or None\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100) # nn.CrossEntropyLoss ignore_index == -100\n",
    "    else:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "# overwrite labels with IGN subwords\n",
    "labels = [index2tag[l] if l!= -100 else \"IGN\" for l in label_ids]\n",
    "\n",
    "pd.DataFrame([tokens, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define single fucntion to wrap this logic for all Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-e6ef8b35dff3ad5e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-1d541dafb6474cb4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-122fb3d1444829cb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "words=['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
      "Reference ner_tags:  [0, 0, 0, 0, 0, 5, 6, 6, 0]\n",
      "\n",
      "tokens=['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '</s>']\n",
      "Enhanced labels:  [-100, 0, 0, 0, -100, -100, -100, 0, 0, 5, 6, 6, -100, 0, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\" examples - Dataset (set of all examples)\n",
    "    # examples[0] - dict | keys='tokens' 'ner_tags' 'langs' 'ner_tags_str'\n",
    "    \"\"\"\n",
    "\n",
    "    # {input_ids : [[0, 96, ..], [0, 25 ..]] attention_mask: [[1..1], [1..1]]}\n",
    "    tokenized_inputs = xlmr_tokenizer(examples['tokens'],\n",
    "                                      truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    new_labels = []\n",
    "    for idx, label in enumerate(examples['ner_tags']):\n",
    "\n",
    "        # [None 0, 1, 1, 2, 3, 3, 3, 4, 5, None]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # set -100 for None or subseq-subwords\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # [ [-100, 5, -100, -100, 6, 6, -100 ..], ...]\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])\n",
    "\n",
    "# encode FR corpus\n",
    "panx_fr_encoded = encode_panx_dataset(panx_ch['fr'])\n",
    "\n",
    "print(f'\\n{words=}')\n",
    "print('Reference ner_tags: ', panx_fr['train'][200]['ner_tags'])\n",
    "print(f'\\n{tokens=}')\n",
    "print('Enhanced labels: ', panx_fr_encoded['train'][200]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "We can measure **word-level** or whole **sequence-level** performance\n",
    "\n",
    "## Word-level performance\n",
    "\n",
    "To measure words in sequence, predictions need to be **list of lists**. Let's align predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def align_predictions(logits, label_ids):\n",
    "    # logits    (B, N, K)\n",
    "    # label_ids (B, N)\n",
    "    preds = np.argmax(logits, axis=2) # (B, N)\n",
    "    bs, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(bs):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    # preds_list = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], ... [\"B-PER\", \"I-PER\"]]\n",
    "    # labels_list = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], .. [\"B-PER\", \"I-PER\"]]\n",
    "    return preds_list, labels_list\n",
    "\n",
    "# example\n",
    "y_true = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune XMLRoBERTa\n",
    "\n",
    "As GERMAN is more representable, let's fine-tune base model on the PAN-X.de.\n",
    "\n",
    "Then we will evaluate its zero-shot cross-lingual performance on FR, IT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from huggingface_hub import notebook_login\n",
    "# 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\n",
    "notebook_login() \n",
    "\n",
    "panx_de_encoded = encode_panx_dataset(panx_ch['de'])\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "log_steps = len(panx_de_encoded['train']) // batch_size\n",
    "model_name = f'{xlmr_model_name}-finetuned-panx-de'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=log_steps,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "# tell Trainer how to compute metrics on the validation set:\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true=y_true, y_pred=y_pred)}\n",
    "\n",
    "# define data collator to PAD each input sequence to the largest sequence length\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    return (MyXLMRoBERTaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xmlr_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_de_encoded['train'],\n",
    "                  eval_dataset=panx_de_encoded['validation'],\n",
    "                  tokenizer=xlmr_tokenizer\n",
    "                  )\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "    trainer.save_model('colab_result') # .save_model(\"/path/to/save\")\n",
    "else:\n",
    "    model = model_init()\n",
    "    model.from_pretrained('colab_result')\n",
    "\n",
    "\n",
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
