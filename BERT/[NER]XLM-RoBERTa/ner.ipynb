{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan : NER with XLM-RoBERTa\n",
    "\n",
    "Task: **perform NER for a Switzerland user** \n",
    "\n",
    "input in Switzerland = 63% DE + 23% FR + 8% IT + 6% EN\n",
    "\n",
    "We will:\n",
    "- import XLM-RoBERTa (Cross-Language Model based on RoBERTa)\n",
    "- add token-classification head for Named Entity Recognition(NER) \n",
    "- Fine-tune multi-ling PANX dataset \n",
    "\n",
    "\n",
    "### Notebook step by step:\n",
    "1. Load 4 datasets: load_dataset('xtreme', name=lang), lang in [de, fr, it, en]\n",
    "2. create custom myXLMRoBERTaForTokenClassification\n",
    "3. Prepare Batches of Training Data : tokenize, tag as IGN subseq-subwords\n",
    "4. Fine tune XML-RoBERTa to PANX.de (aka train)\n",
    "5. Cross-Lingual Transfer: use DE-fine-tuned model to zero-shot on FR,IT and ENG\n",
    "6. Analyze DE zero-shot: FR-fine-tune f1 score as func of train_examples\n",
    "7. Fine-tunning on Multiple Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging # hide logs like cache import\n",
    "\n",
    "logging.getLogger('datasets.builder').setLevel(logging.ERROR)\n",
    "logging.getLogger('datasets.arrow_dataset').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    ValueError(\"Switch to GPU!\")\n",
    "\n",
    "# Install packages\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    packages = [\"transformers\", \"datasets\", \"accelerate\", \"pynvml\", \"seqeval\"]\n",
    "\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pynvml import *\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "DUMMY = False\n",
    "WORKMODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset : XTREME\n",
    "\n",
    "WikiANN : Wikipedia articles in many languages.\n",
    "Each article is annotated with LOC, PER, ORG in IOB2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6631d8a4b14a5fad3a0d7713f38400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e03f2429ecd410480a9f443fe6815ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd3782c083e4fef8751592a5a8c0552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f08a12d0da4ef2bd4127d06cbe25a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 228 MB.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "langs = [\"de\", \"fr\", \"it\", \"en\"] # CH languages\n",
    "fracs = [0.629, 0.229, 0.084, 0.059] # CH spoken ration\n",
    "panx_ch = defaultdict(DatasetDict) # return DatasetDict if key doesnt exist\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    \n",
    "    # DatasetDict. keys={'train', 'validation', 'test'}, values = Dataset\n",
    "    monolingual_ds = load_dataset(\"xtreme\", name=f'PAN-X.{lang}')\n",
    "\n",
    "    for split in monolingual_ds:\n",
    "        # shuffle to avoid bias & downsample according to spoken ratio\n",
    "        panx_ch[lang][split] = (\n",
    "            monolingual_ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * monolingual_ds[split].num_rows))))\n",
    "        \n",
    "print(print_gpu_utilization())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tokens', ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.'])\n",
      "('ner_tags', [0, 0, 0, 0, 0, 5, 6, 6, 0])\n",
      "('langs', ['fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**</td>\n",
       "      <td>À</td>\n",
       "      <td>l'ouest</td>\n",
       "      <td>par</td>\n",
       "      <td>la</td>\n",
       "      <td>province</td>\n",
       "      <td>de</td>\n",
       "      <td>Banten</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1        2    3   4         5      6       7  8\n",
       "0  **  À  l'ouest  par  la  province     de  Banten  .\n",
       "1   O  O        O    O   O     B-LOC  I-LOC   I-LOC  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in panx_ch['fr']['train'][200].items():\n",
    "    print(item)\n",
    "\n",
    "# ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "\n",
    "def create_tag_names(batch):\n",
    "    return {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "\n",
    "panx_fr = panx_ch['fr'].map(create_tag_names)\n",
    "fr_example = panx_fr['train'][200]\n",
    "pd.DataFrame([fr_example['tokens'], fr_example['ner_tags_str']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Custom RoBERTa-based Model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class MyXLMRoBERTaForTokenClassification(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # init RoBERTa with config (class XLMRobertaConfig)\n",
    "        self.num_labels =  config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # build classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # load body pre-trained weights (RoBERTa) & init classifier weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "\n",
    "        encode = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        \n",
    "        # classification\n",
    "        output = self.dropout(encode[0]) # [CLS]\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # logits (B, N, K)\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states = encode.hidden_states, attentions=encode.attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "\n",
    "index2tag = {idx : tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag : idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "# init xlmr tokenizer\n",
    "xlmr_tokenizer = transformers.AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "\n",
    "# we dont AutoModel.from_pretrained(xmlr_model_name) \n",
    "# because we need to overwrite number of output classes and mappings\n",
    "# thus, we AutoConfig.from_pretrained(..) first then AutoModel.from_pretrained(NewConfig)\n",
    "xmlr_config = transformers.AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                                      num_labels=tags.num_classes,\n",
    "                                                      id2label=index2tag,\n",
    "                                                      label2id=tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's init dummy model and make dummy inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # text = \"Jack Sparrow ..\"\n",
    "\n",
    "    # {'input_ids' : [0, 217, 37 .. 2], 'attention_mask' : [1,1...1]}\n",
    "    tokenizer_output = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # [<s>\t▁Jack\t▁Spar\trow\t...\t</s>]\n",
    "    tokens = tokenizer_output.tokens()\n",
    "\n",
    "    # [0, 217, 37 .. 2]|\n",
    "    input_ids = tokenizer_output.input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # [ [0.3, 0.1, 0.6] , [0.4, 0.9, 0.2] , ... [-0.5, 0.1, -0.3] ]\n",
    "    outputs = model(input_ids)[0] # <- (logits, hidden_state)[0]\n",
    "    print(f'{outputs.shape=}')\n",
    "\n",
    "    # [0, 2, 3, 3, 0, 0, 5, 6, 6, 0, 0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    # [O, B-PER, I-PER, I-PER, O, O, B-LOC, I-LOC, O, O]\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    print_gpu_utilization()\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "if DUMMY:\n",
    "    # MyXLMRoBERTaForTokenClassification.from_pretrained(..) is inherited from RobertaPreTrainedModel\n",
    "    xlmr_model = MyXLMRoBERTaForTokenClassification.from_pretrained(\n",
    "        xlmr_model_name, config=xmlr_config).to(device)\n",
    "\n",
    "    text = \"Jack Sparrow loves New Yourk!\"\n",
    "    tag_text(text, tags, xlmr_model, xlmr_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Before we prepare train Batch, let's check how tokenizer tokenize\n",
    "- Problem: tokenizer splits ONE \"l'ouest\" -> FOUR tokens '▁l', \"'\", 'ou', 'est'\n",
    "- Solution: thus we don't need to NER 3 subwords and need to mask them for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words) = 9\n",
      "words=['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
      "labels=[0, 0, 0, 0, 0, 5, 6, 6, 0]\n",
      "\n",
      "len(tokens) = 16\n",
      "tokens=['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁**</td>\n",
       "      <td>▁À</td>\n",
       "      <td>▁l</td>\n",
       "      <td>'</td>\n",
       "      <td>ou</td>\n",
       "      <td>est</td>\n",
       "      <td>▁par</td>\n",
       "      <td>▁la</td>\n",
       "      <td>▁province</td>\n",
       "      <td>▁de</td>\n",
       "      <td>▁Ban</td>\n",
       "      <td>ten</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1   2   3    4    5    6     7    8          9      10     11   12  \\\n",
       "0  <s>  ▁**  ▁À  ▁l    '   ou  est  ▁par  ▁la  ▁province    ▁de   ▁Ban  ten   \n",
       "1  IGN    O   O   O  IGN  IGN  IGN     O    O      B-LOC  I-LOC  I-LOC  IGN   \n",
       "\n",
       "  13   14    15  \n",
       "0  ▁    .  </s>  \n",
       "1  O  IGN   IGN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not WORKMODE: \n",
    "    ####################################### Tokenize example #######################################\n",
    "    words, labels = fr_example['tokens'], fr_example['ner_tags']\n",
    "    print(f'len(words) = {len(words)}\\n{words=}')\n",
    "    print(f'{labels=}')\n",
    "\n",
    "    # {input_ids : [0, 96, 25, 796, 525 ... ], attention_mask : [1 1 .. 1] }\n",
    "    tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)\n",
    "\n",
    "    # ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est' ... ]\n",
    "    tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input.input_ids)\n",
    "    print(f'\\nlen(tokens) = {len(tokens)}\\n{tokens=}')\n",
    "\n",
    "    ########################## MASK subsequent subwords by setting label = -100 ##########################\n",
    "\n",
    "    # [None 0, 1, 2, 2, 2, 2, 3, 4, 5, 6, 7, 7, 8, 8, None]\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    # set -100 for subseq-subwords or None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            label_ids.append(-100) # nn.CrossEntropyLoss ignore_index == -100\n",
    "        else:\n",
    "            label_ids.append(labels[word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    # overwrite labels with IGN subwords\n",
    "    labels = [index2tag[l] if l!= -100 else \"IGN\" for l in label_ids]\n",
    "\n",
    "    pd.DataFrame([tokens, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Now, let's prepare Train Batch\n",
    "We define single fucntion to wrap the previous logic for all Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\" examples - Dataset (set of all examples)\n",
    "    # examples[0] - dict | keys='tokens' 'ner_tags' 'langs' 'ner_tags_str'\n",
    "    \"\"\"\n",
    "\n",
    "    # {input_ids : [[0, 96, ..], [0, 25 ..]] attention_mask: [[1..1], [1..1]]}\n",
    "    tokenized_inputs = xlmr_tokenizer(examples['tokens'],\n",
    "                                      truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    new_labels = []\n",
    "    for idx, label in enumerate(examples['ner_tags']):\n",
    "\n",
    "        # [None 0, 1, 1, 2, 3, 3, 3, 4, 5, None]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # set -100 for None or subseq-subwords\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # [ [-100, 5, -100, -100, 6, 6, -100 ..], ...]\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "panx_encoded = {}\n",
    "\n",
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])\n",
    "\n",
    "panx_encoded['fr'] = encode_panx_dataset(panx_ch['fr'])\n",
    "\n",
    "\"\"\" BEFORE:\n",
    "words=     ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
    "ner_tags:  [   0,  0,          0,     0,    0,          5,    6,        6,  0 ]\n",
    "\n",
    "AFTER:\n",
    "tokens=          ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁',  '.', '</s>']\n",
    "Encoded labels:  [-100,     0,     0,   0,  -100, -100, -100,      0,      0,          5,      6,      6,   -100,   0, -100,   -100]\n",
    "\"\"\"\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "We can measure **word-level** or whole **sequence-level** performance\n",
    "\n",
    "## Word-level performance\n",
    "\n",
    "To measure words in sequence, predictions need to be **list of lists**. Let's align predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def align_predictions(logits, label_ids):\n",
    "    # logits    (B, N, K)\n",
    "    # label_ids (B, N)\n",
    "    preds = np.argmax(logits, axis=2) # (B, N)\n",
    "    bs, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(bs):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    # preds_list = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], ... [\"B-PER\", \"I-PER\"]]\n",
    "    # labels_list = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], .. [\"B-PER\", \"I-PER\"]]\n",
    "    return preds_list, labels_list\n",
    "\n",
    "# example\n",
    "y_true = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune XMLRoBERTa\n",
    "\n",
    "As GERMAN is more representable, let's fine-tune base model on the PAN-X.de.\n",
    "\n",
    "Then we will evaluate its zero-shot cross-lingual performance on FR, IT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# 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\n",
    "# notebook_login() \n",
    "\n",
    "panx_encoded['de'] = encode_panx_dataset(panx_ch['de'])\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "log_steps = len(panx_encoded['de']['train']) // batch_size\n",
    "model_name = f'{xlmr_model_name}-finetuned-panx-de'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=log_steps,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "# tell Trainer how to compute metrics on the validation set:\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true=y_true, y_pred=y_pred)}\n",
    "\n",
    "# define data collator to PAD each input sequence to the largest sequence length\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    return (MyXLMRoBERTaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xmlr_config)\n",
    "            .to(device))\n",
    "\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_encoded['de']['train'],\n",
    "                  eval_dataset=panx_encoded['de']['validation'],\n",
    "                  tokenizer=xlmr_tokenizer\n",
    "                  )\n",
    "\n",
    "f1 = defaultdict(dict)\n",
    "def get_f1(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WORKMODE:\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        print('Start Training!')\n",
    "        trainer.args.push_to_hub = True\n",
    "        trainer.train() # 10 min on Colab GPU\n",
    "        trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "        trainer.save_model('colab_ner_training') # .save_model(\"/path/to/save\")\n",
    "    else:\n",
    "        print(\"Load model weights!\")\n",
    "        trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "                .from_pretrained('../../docs/colab_training/de')\n",
    "                .to(device))\n",
    "        \n",
    "    print_gpu_utilization()\n",
    "\n",
    "    text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "    tag_text(text_de, tags, trainer.model, xlmr_tokenizer)\n",
    "\n",
    "    f1[\"de\"][\"de\"] = get_f1(trainer, dataset=panx_encoded['de'][\"test\"])\n",
    "    print(f\"{f1['de']['de']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NER tags, It works!\n",
    "\n",
    "87% f1 DE test: Nice.\n",
    "\n",
    "Now let's move on to Miltu Languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross-Lingual Transfer\n",
    "\n",
    "Let's quickly shot FR on DE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\n",
    "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)\n",
    "\n",
    "text_fake = \"Nikita poshel v magazin v Amerike i uvidel druga iz Googla\"\n",
    "tag_text(text_fake, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "1. Model managed to zero-transfer LOC \"_Kali\"(DE) to \"_Cali\"(FR).\n",
    "2. zero-shot for non-existent language \"Ruskiy\" (Russian using Latin letters) also does good NER\n",
    "\n",
    "\n",
    "Let's evaluate DE-model on FR, IT and EN test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_lang = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1(trainer, dataset=panx_lang[\"test\"])\n",
    "\n",
    "f1['de']['de'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['de']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['de']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['de']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['de']['de']=}\") # 0.867\n",
    "print(f\"{f1['de']['fr']=}\") # 0.705\n",
    "print(f\"{f1['de']['it']=}\") # 0.675\n",
    "print(f\"{f1['de']['en']=}\") # 0.586\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% and 67% good FR/IT zero-shot transfer. The model has never seen a single labeled Frech or Italian example.\n",
    "\n",
    "58% is okay, but ENG shot from DE expected to be higher.\n",
    "\n",
    "Let's now analyze how good was DE zero-shot on FR:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine-tune FR on 250-500-1k-2k-4k examples\n",
    "\n",
    "Let's fine-tune base XLMRoBERTa on FR using different len of train dataset to compare with DE zero-shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset['train'].shuffle(seed=42).select(range(num_samples))\n",
    "    valid_ds = dataset['validation']\n",
    "    test_ds = dataset['test']\n",
    "\n",
    "    training_args.push_to_hub = False\n",
    "    training_args.logging_steps = len(train_ds)\n",
    "\n",
    "    trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                      data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                      train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    return get_f1(trainer, test_ds)\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    f1s = {}\n",
    "    for ns in [250, 500, 1000, 2000, 4000]:\n",
    "        f1s[ns] = train_on_subset(dataset=panx_encoded['fr'], num_samples=ns)\n",
    "else:\n",
    "    f1s = {\"num_samples\": [250, 500, 1000, 2000, 4000, 15000], \n",
    "          \"f1\": [0.179, 0.604, 0.733, 0.817, 0.838, 0.87]} # result from colab training\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(f1[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\n",
    "metrics_df.set_index(\"num_samples\").plot(ax=ax)\n",
    "plt.legend([\"12k DE Zero-shot\", \"(x)k FR Fine-tuned\"], loc=\"lower right\")\n",
    "plt.ylim((0, 1))\n",
    "plt.title(\"F1 score on PANX.fr[test] by models\")\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"F1 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Fine-tuning on DE + FR Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "panx_encoded['defr'] = DatasetDict()\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    panx_encoded['defr'][split] = concatenate_datasets([panx_encoded['de'][split],  panx_encoded['fr'][split]]).shuffle()\n",
    "\n",
    "\n",
    "training_args.logging_steps = len(panx_encoded['defr']['train']) // batch_size\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer, train_dataset=panx_encoded['defr'][\"train\"],\n",
    "    eval_dataset=panx_encoded['defr'][\"validation\"])\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    trainer.train()\n",
    "    trainer.save_model('defr')\n",
    "else:\n",
    "    trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "             .from_pretrained('../../docs/colab_training/defr')\n",
    "             .to(device))\n",
    "\n",
    "f1['defr']['de'] = evaluate_lang_performance('de', trainer)   \n",
    "f1['defr']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['defr']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['defr']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['defr']['de']=}\") # 0.870\n",
    "print(f\"{f1['defr']['fr']=}\") # 0.887\n",
    "print(f\"{f1['defr']['it']=}\") # 0.814\n",
    "print(f\"{f1['defr']['en']=}\") # 0.677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Fine-tuning on DE + FR + IT + ENG Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_encoded['it'] = encode_panx_dataset(panx_ch['it'])\n",
    "panx_encoded['en'] = encode_panx_dataset(panx_ch['en'])\n",
    "\n",
    "panx_encoded['defriten'] = DatasetDict()\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    panx_encoded['defriten'][split] = concatenate_datasets([panx_encoded['de'][split],  \n",
    "                                                            panx_encoded['fr'][split],\n",
    "                                                            panx_encoded['it'][split],\n",
    "                                                            panx_encoded['en'][split]]).shuffle()\n",
    "\n",
    "\n",
    "training_args.push_to_hub = True\n",
    "training_args.output_dir = \"xml-roberta-base-finetuned-panx-de-fr-it-en\"\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer, train_dataset=panx_encoded['defr'][\"train\"],\n",
    "    eval_dataset=panx_encoded['defr'][\"validation\"])\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    trainer.train()\n",
    "    trainer.save_model('defriten')\n",
    "else:\n",
    "    trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "             .from_pretrained('../../docs/colab_training/defriten')\n",
    "             .to(device))\n",
    "\n",
    "f1['defriten']['de'] = evaluate_lang_performance('de', trainer)   \n",
    "f1['defriten']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['defriten']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['defriten']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['defriten']['de']=}\") # 0.870\n",
    "print(f\"{f1['defriten']['fr']=}\") # 0.887\n",
    "print(f\"{f1['defriten']['it']=}\") # 0.814\n",
    "print(f\"{f1['defriten']['en']=}\") # 0.677"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
