{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan : NER with XLM-RoBERTa\n",
    "\n",
    "Task: **perform NER for a Switzerland customer**\n",
    "\n",
    "We will:\n",
    "- import XLM-RoBERTa (Cross-Language Model based on RoBERTa)\n",
    "- Fine-tune to Named Entity Recognition(NER)\n",
    "\n",
    "Switzerland = 63% DE + 23% FR + 8% IT + 6% EN\n",
    "\n",
    "As DE is most spoken, we will use DE for zero-shot cross-lingual transfer to FR, IT and EN.\n",
    "\n",
    "## Detailed plan\n",
    "1. Load 4 datasets: load_dataset('xtreme', name='fr'). + de + en + it\n",
    "    - inspect one example|\n",
    "2. create custom myXLMRoBERTaForTokenClassification\n",
    "    - define class\n",
    "    - make dummy inference\n",
    "3. Prepare Batches of Training Data\n",
    "    - tokenize\n",
    "    - tag as IGN subsequent subwords (from tokenization)\n",
    "    - remove original 'langs' 'ner_tags' 'tokens' keys from Dataset\n",
    "4. Fine tune XML-RoBERTa to PANX.de\n",
    "    - train\n",
    "5. Cross-Lingual Transfer\n",
    "    - using DE fine-tuned model, zero-shot on FR,IT and ENG\n",
    "6. Analyze how good zero-shot is:\n",
    "    - fine-tune on FR using different nb of training data\n",
    "\n",
    "### Example:\n",
    "\n",
    "['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
    "\n",
    "labels = 0    0   0\t0\t0\t  5 \t  6 \t  6 \t0\n",
    "\n",
    "tags   = O    O   O\tO\tO\tB-LOC\tI-LOC\tI-LOC\tO\n",
    "\n",
    "### Result:\n",
    "\n",
    "tokens=['< s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '< /s>']\n",
    "\n",
    "labels =  [-100, 0, 0, 0, -100, -100, -100, 0, 0, 5, 6, 6, -100, 0, -100, -100]\n",
    "\n",
    "tags   = [IGN, O, O, O, IGN, IGN, IGN, O, O, B-LOC, I-LOC, I-LOC, IGN, O, IGN, IGN]\n",
    "\n",
    "\n",
    "\n",
    "# 1. Dataset : EXTREME\n",
    "\n",
    "WikiANN : Wikipedia articles in many languages.\n",
    "Each article is annotated with LOC, PER, ORG in IOB2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    ValueError(\"Switch to GPU!\")\n",
    "\n",
    "# Install packages\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    packages = [\"transformers\", \"datasets\", \"accelerate\", \"pynvml\", \"seqeval\"]\n",
    "\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pynvml import *\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "from transformers import Trainer\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "DUMMY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc47da00cf43402498fa714f539188de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.de\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-e5ddf09f1ae095ec.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.de\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-25e7e2dd003d0fa6.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.de\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-73a95bc0accfea8b.arrow\n",
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8168cf35b68241258f63367ea2f9ddeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-6ff29513007ec78b.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-c5c9a4fc19dfd7d6.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-9711ab25936b81b7.arrow\n",
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee491b2266a4a198c6096ca584c496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-daa9a1770078307c.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-5e244c05031bab3c.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-497ee15c12bff58d.arrow\n",
      "Found cached dataset xtreme (C:/Users/nikit/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ce6c7f39c44395909f33de0733720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-757845faa9fa6949.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-305cefc7ffa49fd9.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-e5ec5e6ba7c1237d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 135 MB.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "langs = [\"de\", \"fr\", \"it\", \"en\"] # CH languages\n",
    "fracs = [0.629, 0.229, 0.084, 0.059] # CH spoken ration\n",
    "panx_ch = defaultdict(DatasetDict) # return DatasetDict if key doesnt exist\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    \n",
    "    # DatasetDict. keys={'train', 'validation', 'test'}, values = Dataset\n",
    "    monolingual_ds = load_dataset(\"xtreme\", name=f'PAN-X.{lang}')\n",
    "\n",
    "    for split in monolingual_ds:\n",
    "        # shuffle to avoid bias & downsample according to spoken ratio\n",
    "        panx_ch[lang][split] = (\n",
    "            monolingual_ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * monolingual_ds[split].num_rows))))\n",
    "        \n",
    "print(print_gpu_utilization())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Inspect one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tokens', ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.'])\n",
      "('ner_tags', [0, 0, 0, 0, 0, 5, 6, 6, 0])\n",
      "('langs', ['fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr'])\n"
     ]
    }
   ],
   "source": [
    "for item in panx_ch['fr']['train'][200].items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-8d1477a4eff16eb5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-0832c54610a26671.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-af085c7a24eed84e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tags=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**</td>\n",
       "      <td>À</td>\n",
       "      <td>l'ouest</td>\n",
       "      <td>par</td>\n",
       "      <td>la</td>\n",
       "      <td>province</td>\n",
       "      <td>de</td>\n",
       "      <td>Banten</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1        2    3   4         5      6       7  8\n",
       "0  **  À  l'ouest  par  la  province     de  Banten  .\n",
       "1   O  O        O    O   O     B-LOC  I-LOC   I-LOC  O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make tags in human-readable format\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "print(f'\\n{tags=}')\n",
    "\n",
    "def create_tag_names(batch):\n",
    "    return {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "\n",
    "panx_fr = panx_ch['fr'].map(create_tag_names)\n",
    "fr_example = panx_fr['train'][200]\n",
    "pd.DataFrame([fr_example['tokens'], fr_example['ner_tags_str']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Custom RoBERTa-based Model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class MyXLMRoBERTaForTokenClassification(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # init RoBERTa with config (class XLMRobertaConfig)\n",
    "        self.num_labels =  config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # build classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # load body pre-trained weights (RoBERTa) & init classifier weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "\n",
    "        encode = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        \n",
    "        # classification\n",
    "        output = self.dropout(encode[0]) # [CLS]\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # logits (B, N, K)\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states = encode.hidden_states, attentions=encode.attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 135 MB.\n"
     ]
    }
   ],
   "source": [
    "index2tag = {idx : tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag : idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "# init xlmr tokenizer\n",
    "xlmr_tokenizer = transformers.AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "\n",
    "# we dont AutoModel.from_pretrained(xmlr_model_name) \n",
    "# because we need to overwrite number of output classes and mappings\n",
    "# thus, we AutoConfig.from_pretrained(..) first then AutoModel.from_pretrained(NewConfig)\n",
    "xmlr_config = transformers.AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                                      num_labels=tags.num_classes,\n",
    "                                                      id2label=index2tag,\n",
    "                                                      label2id=tag2index)\n",
    "\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's init dummy model and make dummy inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # text = \"Jack Sparrow ..\"\n",
    "\n",
    "    # {'input_ids' : [0, 217, 37 .. 2], 'attention_mask' : [1,1...1]}\n",
    "    tokenizer_output = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # [<s>\t▁Jack\t▁Spar\trow\t...\t</s>]\n",
    "    tokens = tokenizer_output.tokens()\n",
    "\n",
    "    # [0, 217, 37 .. 2]|\n",
    "    input_ids = tokenizer_output.input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # [ [0.3, 0.1, 0.6] , [0.4, 0.9, 0.2] , ... [-0.5, 0.1, -0.3] ]\n",
    "    outputs = model(input_ids)[0] # <- (logits, hidden_state)[0]\n",
    "    print(f'{outputs.shape=}')\n",
    "\n",
    "    # [0, 2, 3, 3, 0, 0, 5, 6, 6, 0, 0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    # [O, B-PER, I-PER, I-PER, O, O, B-LOC, I-LOC, O, O]\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    print_gpu_utilization()\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "if DUMMY:\n",
    "    # MyXLMRoBERTaForTokenClassification.from_pretrained(..) is inherited from RobertaPreTrainedModel\n",
    "    xlmr_model = MyXLMRoBERTaForTokenClassification.from_pretrained(\n",
    "        xlmr_model_name, config=xmlr_config).to(device)\n",
    "\n",
    "    text = \"Jack Sparrow loves New Yourk!\"\n",
    "    tag_text(text, tags, xlmr_model, xlmr_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU memory occupied: 1337 MB.\n",
    "outputs.shape=torch.Size([1, 11, 7])\n",
    "GPU memory occupied: 2147 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Now, let's prepare Train Batch\n",
    "\n",
    "### First, let's check how tokenizer tokenize\n",
    "- tokenizer splits ONE \"l'ouest\" -> FOUR tokens '▁l', \"'\", 'ou', 'est'\n",
    "- thus we don't need to NER 3 subwords and need to mask them for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words) = 9\n",
      "words=['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
      "labels=[0, 0, 0, 0, 0, 5, 6, 6, 0]\n",
      "\n",
      "len(tokens) = 16\n",
      "tokens=['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁**</td>\n",
       "      <td>▁À</td>\n",
       "      <td>▁l</td>\n",
       "      <td>'</td>\n",
       "      <td>ou</td>\n",
       "      <td>est</td>\n",
       "      <td>▁par</td>\n",
       "      <td>▁la</td>\n",
       "      <td>▁province</td>\n",
       "      <td>▁de</td>\n",
       "      <td>▁Ban</td>\n",
       "      <td>ten</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1   2   3    4    5    6     7    8          9      10     11   12  \\\n",
       "0  <s>  ▁**  ▁À  ▁l    '   ou  est  ▁par  ▁la  ▁province    ▁de   ▁Ban  ten   \n",
       "1  IGN    O   O   O  IGN  IGN  IGN     O    O      B-LOC  I-LOC  I-LOC  IGN   \n",
       "\n",
       "  13   14    15  \n",
       "0  ▁    .  </s>  \n",
       "1  O  IGN   IGN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "words, labels = fr_example['tokens'], fr_example['ner_tags']\n",
    "print(f'len(words) = {len(words)}\\n{words=}')\n",
    "print(f'{labels=}')\n",
    "\n",
    "# {input_ids : [0, 96, 25, 796, 525 ... ], attention_mask : [1 1 .. 1] }\n",
    "tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)\n",
    "\n",
    "# ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est' ... ]\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input.input_ids)\n",
    "print(f'\\nlen(tokens) = {len(tokens)}\\n{tokens=}')\n",
    "\n",
    "# MASK subsequent subwords by setting label = -100\n",
    "\n",
    "# [None 0, 1, 2, 2, 2, 2, 3, 4, 5, 6, 7, 7, 8, 8, None]\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "# set -100 for subseq-subwords or None\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100) # nn.CrossEntropyLoss ignore_index == -100\n",
    "    else:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "# overwrite labels with IGN subwords\n",
    "labels = [index2tag[l] if l!= -100 else \"IGN\" for l in label_ids]\n",
    "\n",
    "pd.DataFrame([tokens, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define single fucntion to wrap this logic for all Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-e6ef8b35dff3ad5e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-1d541dafb6474cb4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-122fb3d1444829cb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "words=['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
      "Reference ner_tags:  [0, 0, 0, 0, 0, 5, 6, 6, 0]\n",
      "\n",
      "tokens=['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '</s>']\n",
      "Enhanced labels:  [-100, 0, 0, 0, -100, -100, -100, 0, 0, 5, 6, 6, -100, 0, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\" examples - Dataset (set of all examples)\n",
    "    # examples[0] - dict | keys='tokens' 'ner_tags' 'langs' 'ner_tags_str'\n",
    "    \"\"\"\n",
    "\n",
    "    # {input_ids : [[0, 96, ..], [0, 25 ..]] attention_mask: [[1..1], [1..1]]}\n",
    "    tokenized_inputs = xlmr_tokenizer(examples['tokens'],\n",
    "                                      truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    new_labels = []\n",
    "    for idx, label in enumerate(examples['ner_tags']):\n",
    "\n",
    "        # [None 0, 1, 1, 2, 3, 3, 3, 4, 5, None]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # set -100 for None or subseq-subwords\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # [ [-100, 5, -100, -100, 6, 6, -100 ..], ...]\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])\n",
    "\n",
    "# encode FR corpus\n",
    "panx_fr_encoded = encode_panx_dataset(panx_ch['fr'])\n",
    "\n",
    "print(f'\\n{words=}')\n",
    "print('Reference ner_tags: ', panx_fr['train'][200]['ner_tags'])\n",
    "print(f'\\n{tokens=}')\n",
    "print('Enhanced labels: ', panx_fr_encoded['train'][200]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "We can measure **word-level** or whole **sequence-level** performance\n",
    "\n",
    "## Word-level performance\n",
    "\n",
    "To measure words in sequence, predictions need to be **list of lists**. Let's align predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def align_predictions(logits, label_ids):\n",
    "    # logits    (B, N, K)\n",
    "    # label_ids (B, N)\n",
    "    preds = np.argmax(logits, axis=2) # (B, N)\n",
    "    bs, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(bs):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    # preds_list = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], ... [\"B-PER\", \"I-PER\"]]\n",
    "    # labels_list = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], .. [\"B-PER\", \"I-PER\"]]\n",
    "    return preds_list, labels_list\n",
    "\n",
    "# example\n",
    "y_true = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune XMLRoBERTa\n",
    "\n",
    "As GERMAN is more representable, let's fine-tune base model on the PAN-X.de.\n",
    "\n",
    "Then we will evaluate its zero-shot cross-lingual performance on FR, IT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from huggingface_hub import notebook_login\n",
    "# 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\n",
    "notebook_login() \n",
    "\n",
    "panx_de_encoded = encode_panx_dataset(panx_ch['de'])\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "log_steps = len(panx_de_encoded['train']) // batch_size\n",
    "model_name = f'{xlmr_model_name}-finetuned-panx-de'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=log_steps,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "# tell Trainer how to compute metrics on the validation set:\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true=y_true, y_pred=y_pred)}\n",
    "\n",
    "# define data collator to PAD each input sequence to the largest sequence length\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    return (MyXLMRoBERTaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xmlr_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WORKSPACE\\nlphub\\BERT\\[NER]XLM-RoBERTa\\xlm-roberta-base-finetuned-panx-de is already a clone of https://huggingface.co/nikitakapitan/xlm-roberta-base-finetuned-panx-de. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 3951 MB.\n",
      "outputs.shape=torch.Size([1, 14, 7])\n",
      "GPU memory occupied: 4317 MB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁ist</td>\n",
       "      <td>▁ein</td>\n",
       "      <td>▁Informati</td>\n",
       "      <td>ker</td>\n",
       "      <td>▁bei</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁Kaliforni</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4     5           6    7     8        9   \\\n",
       "Tokens  <s>  ▁Jeff    ▁De     an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google   \n",
       "Tags      O  B-PER  I-PER  I-PER     O     O           O    O     O    B-ORG   \n",
       "\n",
       "         10          11     12    13  \n",
       "Tokens  ▁in  ▁Kaliforni     en  </s>  \n",
       "Tags      O       B-LOC  I-LOC     O  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_de_encoded['train'],\n",
    "                  eval_dataset=panx_de_encoded['validation'],\n",
    "                  tokenizer=xlmr_tokenizer\n",
    "                  )\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    trainer.train() # 10 min on Colab GPU\n",
    "    trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "    trainer.save_model('colab_ner_training') # .save_model(\"/path/to/save\")\n",
    "else:\n",
    "    trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "             .from_pretrained('../../docs/colab_ner_training')\n",
    "             .to(device))\n",
    "    \n",
    "print_gpu_utilization()\n",
    "\n",
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Let's compute f1 score for DE test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec0268342ff48268cb82990ba28a87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1['de']['de']=0.8672356299101339\n"
     ]
    }
   ],
   "source": [
    "def get_f1(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]\n",
    "\n",
    "f1 = defaultdict(dict)\n",
    "f1[\"de\"][\"de\"] = get_f1(trainer, dataset=panx_de_encoded[\"test\"])\n",
    "print(f\"{f1['de']['de']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87% Nice.\n",
    "\n",
    "Now let's move on to Miltu Languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross-Lingual Transfer\n",
    "\n",
    "Let's quickly shot FR on DE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape=torch.Size([1, 14, 7])\n",
      "GPU memory occupied: 4317 MB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁est</td>\n",
       "      <td>▁informatic</td>\n",
       "      <td>ien</td>\n",
       "      <td>▁chez</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁en</td>\n",
       "      <td>▁Cali</td>\n",
       "      <td>for</td>\n",
       "      <td>nie</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4            5    6      7        8    9   \\\n",
       "Tokens  <s>  ▁Jeff    ▁De     an  ▁est  ▁informatic  ien  ▁chez  ▁Google  ▁en   \n",
       "Tags      O  B-PER  I-PER  I-PER     O            O    O      O    B-ORG    O   \n",
       "\n",
       "           10     11     12    13  \n",
       "Tokens  ▁Cali    for    nie  </s>  \n",
       "Tags    B-LOC  B-LOC  I-LOC     O  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\n",
    "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Model managed to zero-transfer LOC \"_Kali\"(DE) to \"_Cali\"(FR).\n",
    "\n",
    "Let's zero-shot for non-existent language \"Ruskiy\" (Russian using Latin letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape=torch.Size([1, 17, 7])\n",
      "GPU memory occupied: 4319 MB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Nik</td>\n",
       "      <td>ita</td>\n",
       "      <td>▁pos</td>\n",
       "      <td>hel</td>\n",
       "      <td>▁v</td>\n",
       "      <td>▁magazin</td>\n",
       "      <td>▁v</td>\n",
       "      <td>▁Amerike</td>\n",
       "      <td>▁i</td>\n",
       "      <td>▁uvid</td>\n",
       "      <td>el</td>\n",
       "      <td>▁druga</td>\n",
       "      <td>▁iz</td>\n",
       "      <td>▁Googl</td>\n",
       "      <td>a</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2     3    4   5         6   7         8   9      10  \\\n",
       "Tokens  <s>   ▁Nik    ita  ▁pos  hel  ▁v  ▁magazin  ▁v  ▁Amerike  ▁i  ▁uvid   \n",
       "Tags      O  B-PER  B-PER     O    O   O         O   O     B-LOC   O      O   \n",
       "\n",
       "        11      12   13      14 15    16  \n",
       "Tokens  el  ▁druga  ▁iz  ▁Googl  a  </s>  \n",
       "Tags     O       O    O   B-ORG  O     O  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_nik = \"Nikita poshel v magazin v Amerike i uvidel druga iz Googla\"\n",
    "tag_text(text_nik, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive. Even for fake inexistent languages, the model still does good NER\n",
    "\n",
    "Let's evaluate the whole test dataset FR, IT and EN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-9e1b1130e238a020.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-7051d5768c6d3384.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.fr\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-5166b31a5bf7d5cd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e79798fdf641b79e603e1dc8288806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-7cf120e614ad7c4c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.it\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-4314f7dda1a64fd7.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2d01f246fe4da5bacfd8712592556d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b760e22894ec49cbaf0e80d8668b5b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-29e2643797d361bc.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-3fd4586a43bf3d39.arrow\n",
      "Loading cached processed dataset at C:\\Users\\nikit\\.cache\\huggingface\\datasets\\xtreme\\PAN-X.en\\1.0.0\\29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4\\cache-0e7204a0d6b3c3e3.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1f5e29a9b74e0eb6428f8c1933f8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1['de']['de']=0.8672356299101339\n",
      "f1['de']['fr']=0.705163262023484\n",
      "f1['de']['it']=0.6753794266441822\n",
      "f1['de']['en']=0.586381541924592\n"
     ]
    }
   ],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_lang = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1(trainer, dataset=panx_lang[\"test\"])\n",
    "\n",
    "f1['de']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['de']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['de']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['de']['de']=}\")\n",
    "print(f\"{f1['de']['fr']=}\")\n",
    "print(f\"{f1['de']['it']=}\")\n",
    "print(f\"{f1['de']['en']=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% and 67% good FR/IT zero-shot transfer. The model has never seen a single labeled Frech or Italian example.\n",
    "\n",
    "58% is okay, but ENG shot from DE expected to be higher.\n",
    "\n",
    "Let's now analyze how good is this DE zero-shot on FR:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine-tune FR\n",
    "\n",
    "Let's fine-tune base XLMRoBERTa on FR using different len of train dataset to compare with DE zero-shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset['train'].shuffle(seed=42).select(range(num_samples))\n",
    "    valid_ds = dataset['validation']\n",
    "    test_ds = dataset['test']\n",
    "\n",
    "    training_args.logging_steps = len(train_ds)\n",
    "\n",
    "    trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                      data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                      train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    f1 = get_f1(trainer, test_ds)\n",
    "    print_gpu_utilization()\n",
    "    return pd.DataFrame.from_dict({'num_samples':[len(train_ds)], 'f1': [f1]})\n",
    "\n",
    "train_on_subset(dataset=panx_fr_encoded, num_samples=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
