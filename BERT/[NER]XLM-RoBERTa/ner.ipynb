{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan : NER with XLM-RoBERTa\n",
    "\n",
    "Task: **perform NER for a Switzerland user** \n",
    "\n",
    "input in Switzerland = 63% DE + 23% FR + 8% IT + 6% EN\n",
    "\n",
    "We will:\n",
    "- import XLM-RoBERTa (Cross-Language Model based on RoBERTa)\n",
    "- add token-classification head for Named Entity Recognition(NER) \n",
    "- Fine-tune multi-ling PANX dataset \n",
    "\n",
    "\n",
    "### Notebook step by step:\n",
    "1. Load 4 datasets: load_dataset('xtreme', name=lang), lang in [de, fr, it, en]\n",
    "2. create custom myXLMRoBERTaForTokenClassification\n",
    "3. Prepare Batches of Training Data : tokenize, tag as IGN subseq-subwords\n",
    "4. Fine tune XML-RoBERTa to PANX.de (aka train)\n",
    "5. Cross-Lingual Transfer: use DE-fine-tuned model to zero-shot on FR,IT and ENG\n",
    "6. Analyze DE zero-shot: FR-fine-tune f1 score as func of train_examples\n",
    "7. Fine-tunning on Multiple Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging # hide logs like cache import\n",
    "\n",
    "logging.getLogger('datasets.builder').setLevel(logging.ERROR)\n",
    "logging.getLogger('datasets.arrow_dataset').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    ValueError(\"Switch to GPU!\")\n",
    "\n",
    "# Install packages\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    packages = [\"transformers\", \"datasets\", \"accelerate\", \"pynvml\", \"seqeval\"]\n",
    "\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pynvml import *\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "DUMMY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset : XTREME\n",
    "\n",
    "WikiANN : Wikipedia articles in many languages.\n",
    "Each article is annotated with LOC, PER, ORG in IOB2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c434ba1d4414b37aca999dda82f31b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7938a85dab8c43f096970b72e5d04efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051daa88844e4b76bad78e4a54be92a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad8c6e288534733a61f96d51d2a380a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 281 MB.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "langs = [\"de\", \"fr\", \"it\", \"en\"] # CH languages\n",
    "fracs = [0.629, 0.229, 0.084, 0.059] # CH spoken ration\n",
    "panx_ch = defaultdict(DatasetDict) # return DatasetDict if key doesnt exist\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    \n",
    "    # DatasetDict. keys={'train', 'validation', 'test'}, values = Dataset\n",
    "    monolingual_ds = load_dataset(\"xtreme\", name=f'PAN-X.{lang}')\n",
    "\n",
    "    for split in monolingual_ds:\n",
    "        # shuffle to avoid bias & downsample according to spoken ratio\n",
    "        panx_ch[lang][split] = (\n",
    "            monolingual_ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * monolingual_ds[split].num_rows))))\n",
    "        \n",
    "print(print_gpu_utilization())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tokens', ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.'])\n",
      "('ner_tags', [0, 0, 0, 0, 0, 5, 6, 6, 0])\n",
      "('langs', ['fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr', 'fr'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**</td>\n",
       "      <td>À</td>\n",
       "      <td>l'ouest</td>\n",
       "      <td>par</td>\n",
       "      <td>la</td>\n",
       "      <td>province</td>\n",
       "      <td>de</td>\n",
       "      <td>Banten</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1        2    3   4         5      6       7  8\n",
       "0  **  À  l'ouest  par  la  province     de  Banten  .\n",
       "1   O  O        O    O   O     B-LOC  I-LOC   I-LOC  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in panx_ch['fr']['train'][200].items():\n",
    "    print(item)\n",
    "\n",
    "# ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "\n",
    "def create_tag_names(batch):\n",
    "    return {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "\n",
    "panx_fr = panx_ch['fr'].map(create_tag_names)\n",
    "fr_example = panx_fr['train'][200]\n",
    "pd.DataFrame([fr_example['tokens'], fr_example['ner_tags_str']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Custom RoBERTa-based Model for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class MyXLMRoBERTaForTokenClassification(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # init RoBERTa with config (class XLMRobertaConfig)\n",
    "        self.num_labels =  config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # build classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # load body pre-trained weights (RoBERTa) & init classifier weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "\n",
    "        encode = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        \n",
    "        # classification\n",
    "        output = self.dropout(encode[0]) # [CLS]\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # logits (B, N, K)\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states = encode.hidden_states, attentions=encode.attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "tags = panx_ch['fr']['train'].features['ner_tags'].feature\n",
    "\n",
    "index2tag = {idx : tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag : idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "# init xlmr tokenizer\n",
    "xlmr_tokenizer = transformers.AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "\n",
    "# we dont AutoModel.from_pretrained(xmlr_model_name) \n",
    "# because we need to overwrite number of output classes and mappings\n",
    "# thus, we AutoConfig.from_pretrained(..) first then AutoModel.from_pretrained(NewConfig)\n",
    "xmlr_config = transformers.AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                                      num_labels=tags.num_classes,\n",
    "                                                      id2label=index2tag,\n",
    "                                                      label2id=tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's init dummy model and make dummy inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # text = \"Jack Sparrow ..\"\n",
    "\n",
    "    # {'input_ids' : [0, 217, 37 .. 2], 'attention_mask' : [1,1...1]}\n",
    "    tokenizer_output = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # [<s>\t▁Jack\t▁Spar\trow\t...\t</s>]\n",
    "    tokens = tokenizer_output.tokens()\n",
    "\n",
    "    # [0, 217, 37 .. 2]|\n",
    "    input_ids = tokenizer_output.input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # [ [0.3, 0.1, 0.6] , [0.4, 0.9, 0.2] , ... [-0.5, 0.1, -0.3] ]\n",
    "    outputs = model(input_ids)[0] # <- (logits, hidden_state)[0]\n",
    "    print(f'{outputs.shape=}')\n",
    "\n",
    "    # [0, 2, 3, 3, 0, 0, 5, 6, 6, 0, 0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    # [O, B-PER, I-PER, I-PER, O, O, B-LOC, I-LOC, O, O]\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    print_gpu_utilization()\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "if DUMMY:\n",
    "    # MyXLMRoBERTaForTokenClassification.from_pretrained(..) is inherited from RobertaPreTrainedModel\n",
    "    xlmr_model = MyXLMRoBERTaForTokenClassification.from_pretrained(\n",
    "        xlmr_model_name, config=xmlr_config).to(device)\n",
    "\n",
    "    text = \"Jack Sparrow loves New Yourk!\"\n",
    "    tag_text(text, tags, xlmr_model, xlmr_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Before we prepare train Batch, let's check how tokenizer tokenize\n",
    "- Problem: tokenizer splits ONE \"l'ouest\" -> FOUR tokens '▁l', \"'\", 'ou', 'est'\n",
    "- Solution: thus we don't need to NER 3 subwords and need to mask them for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words) = 9\n",
      "words=['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
      "labels=[0, 0, 0, 0, 0, 5, 6, 6, 0]\n",
      "\n",
      "len(tokens) = 16\n",
      "tokens=['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁**</td>\n",
       "      <td>▁À</td>\n",
       "      <td>▁l</td>\n",
       "      <td>'</td>\n",
       "      <td>ou</td>\n",
       "      <td>est</td>\n",
       "      <td>▁par</td>\n",
       "      <td>▁la</td>\n",
       "      <td>▁province</td>\n",
       "      <td>▁de</td>\n",
       "      <td>▁Ban</td>\n",
       "      <td>ten</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1   2   3    4    5    6     7    8          9      10     11   12  \\\n",
       "0  <s>  ▁**  ▁À  ▁l    '   ou  est  ▁par  ▁la  ▁province    ▁de   ▁Ban  ten   \n",
       "1  IGN    O   O   O  IGN  IGN  IGN     O    O      B-LOC  I-LOC  I-LOC  IGN   \n",
       "\n",
       "  13   14    15  \n",
       "0  ▁    .  </s>  \n",
       "1  O  IGN   IGN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################### Tokenize example #######################################\n",
    "words, labels = fr_example['tokens'], fr_example['ner_tags']\n",
    "print(f'len(words) = {len(words)}\\n{words=}')\n",
    "print(f'{labels=}')\n",
    "\n",
    "# {input_ids : [0, 96, 25, 796, 525 ... ], attention_mask : [1 1 .. 1] }\n",
    "tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)\n",
    "\n",
    "# ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est' ... ]\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input.input_ids)\n",
    "print(f'\\nlen(tokens) = {len(tokens)}\\n{tokens=}')\n",
    "\n",
    "########################## MASK subsequent subwords by setting label = -100 ##########################\n",
    "\n",
    "# [None 0, 1, 2, 2, 2, 2, 3, 4, 5, 6, 7, 7, 8, 8, None]\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "# set -100 for subseq-subwords or None\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100) # nn.CrossEntropyLoss ignore_index == -100\n",
    "    else:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "# overwrite labels with IGN subwords\n",
    "labels = [index2tag[l] if l!= -100 else \"IGN\" for l in label_ids]\n",
    "\n",
    "pd.DataFrame([tokens, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Now, let's prepare Train Batch\n",
    "We define single fucntion to wrap the previous logic for all Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\" examples - Dataset (set of all examples)\n",
    "    # examples[0] - dict | keys='tokens' 'ner_tags' 'langs' 'ner_tags_str'\n",
    "    \"\"\"\n",
    "\n",
    "    # {input_ids : [[0, 96, ..], [0, 25 ..]] attention_mask: [[1..1], [1..1]]}\n",
    "    tokenized_inputs = xlmr_tokenizer(examples['tokens'],\n",
    "                                      truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    new_labels = []\n",
    "    for idx, label in enumerate(examples['ner_tags']):\n",
    "\n",
    "        # [None 0, 1, 1, 2, 3, 3, 3, 4, 5, None]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # set -100 for None or subseq-subwords\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # [ [-100, 5, -100, -100, 6, 6, -100 ..], ...]\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "panx_encoded = {}\n",
    "\n",
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])\n",
    "\n",
    "panx_encoded['fr'] = encode_panx_dataset(panx_ch['fr'])\n",
    "\n",
    "\"\"\" BEFORE:\n",
    "words=     ['**', 'À', \"l'ouest\", 'par', 'la', 'province', 'de', 'Banten', '.']\n",
    "ner_tags:  [   0,  0,          0,     0,    0,          5,    6,        6,  0 ]\n",
    "\n",
    "AFTER:\n",
    "tokens=          ['<s>', '▁**', '▁À', '▁l', \"'\", 'ou', 'est', '▁par', '▁la', '▁province', '▁de', '▁Ban', 'ten', '▁',  '.', '</s>']\n",
    "Encoded labels:  [-100,     0,     0,   0,  -100, -100, -100,      0,      0,          5,      6,      6,   -100,   0, -100,   -100]\n",
    "\"\"\"\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "We can measure **word-level** or whole **sequence-level** performance\n",
    "\n",
    "## Word-level performance\n",
    "\n",
    "To measure words in sequence, predictions need to be **list of lists**. Let's align predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def align_predictions(logits, label_ids):\n",
    "    # logits    (B, N, K)\n",
    "    # label_ids (B, N)\n",
    "    preds = np.argmax(logits, axis=2) # (B, N)\n",
    "    bs, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(bs):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    # preds_list = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], ... [\"B-PER\", \"I-PER\"]]\n",
    "    # labels_list = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], .. [\"B-PER\", \"I-PER\"]]\n",
    "    return preds_list, labels_list\n",
    "\n",
    "# example\n",
    "y_true = [[\"O\", \"B-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune XMLRoBERTa\n",
    "\n",
    "As GERMAN is more representable, let's fine-tune base model on the PAN-X.de.\n",
    "\n",
    "Then we will evaluate its zero-shot cross-lingual performance on FR, IT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da3e22c670e455ab8c8b428192bc5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "os.environ['HUGGINGFACE_TOKEN'] = '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'\n",
    "notebook_login() \n",
    "\n",
    "panx_de_encoded = encode_panx_dataset(panx_ch['de'])\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "log_steps = len(panx_de_encoded['train']) // batch_size\n",
    "model_name = f'{xlmr_model_name}-finetuned-panx-de'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=log_steps,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "# tell Trainer how to compute metrics on the validation set:\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true=y_true, y_pred=y_pred)}\n",
    "\n",
    "# define data collator to PAD each input sequence to the largest sequence length\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    return (MyXLMRoBERTaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xmlr_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model weights!\n",
      "GPU memory occupied: 6131 MB.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m print_gpu_utilization()\n\u001b[0;32m     26\u001b[0m text_de \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mJeff Dean ist ein Informatiker bei Google in Kalifornien\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m tag_text(text_de, tags, trainer\u001b[39m.\u001b[39;49mmodel, xlmr_tokenizer)\n",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m, in \u001b[0;36mtag_text\u001b[1;34m(text, tags, model, tokenizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[39m# [ [0.3, 0.1, 0.6] , [0.4, 0.9, 0.2] , ... [-0.5, 0.1, -0.3] ]\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids)[\u001b[39m0\u001b[39m] \u001b[39m# <- (logits, hidden_state)[0]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00moutputs\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[39m# [0, 2, 3, 3, 0, 0, 5, 6, 6, 0, 0]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mMyXLMRoBERTaForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, labels, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 20\u001b[0m     encode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     21\u001b[0m                            token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     24\u001b[0m     \u001b[39m# classification\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(encode[\u001b[39m0\u001b[39m]) \u001b[39m# [CLS]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    853\u001b[0m     embedding_output,\n\u001b[0;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    863\u001b[0m )\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    528\u001b[0m         hidden_states,\n\u001b[0;32m    529\u001b[0m         attention_mask,\n\u001b[0;32m    530\u001b[0m         layer_head_mask,\n\u001b[0;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    533\u001b[0m         past_key_value,\n\u001b[0;32m    534\u001b[0m         output_attentions,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    400\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    401\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    409\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    412\u001b[0m         hidden_states,\n\u001b[0;32m    413\u001b[0m         attention_mask,\n\u001b[0;32m    414\u001b[0m         head_mask,\n\u001b[0;32m    415\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    416\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    418\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    420\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:338\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    329\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    330\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    337\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 338\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    339\u001b[0m         hidden_states,\n\u001b[0;32m    340\u001b[0m         attention_mask,\n\u001b[0;32m    341\u001b[0m         head_mask,\n\u001b[0;32m    342\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    343\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    344\u001b[0m         past_key_value,\n\u001b[0;32m    345\u001b[0m         output_attentions,\n\u001b[0;32m    346\u001b[0m     )\n\u001b[0;32m    347\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    348\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:195\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    186\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    187\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    194\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 195\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[0;32m    197\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_de_encoded['train'],\n",
    "                  eval_dataset=panx_de_encoded['validation'],\n",
    "                  tokenizer=xlmr_tokenizer\n",
    "                  )\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print('Start Training!')\n",
    "    trainer.args.push_to_hub = True\n",
    "    trainer.train() # 10 min on Colab GPU\n",
    "    trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "    trainer.save_model('colab_ner_training') # .save_model(\"/path/to/save\")\n",
    "else:\n",
    "    print(\"Load model weights!\")\n",
    "    trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "             .from_pretrained('../../docs/colab_training/de')\n",
    "             .to(device))\n",
    "    \n",
    "print_gpu_utilization()\n",
    "\n",
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NER tags, It works! Let's compute f1 score for DE test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]\n",
    "\n",
    "f1 = defaultdict(dict)\n",
    "f1[\"de\"][\"de\"] = get_f1(trainer, dataset=panx_de_encoded[\"test\"])\n",
    "print(f\"{f1['de']['de']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87% Nice.\n",
    "\n",
    "Now let's move on to Miltu Languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross-Lingual Transfer\n",
    "\n",
    "Let's quickly shot FR on DE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\n",
    "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)\n",
    "\n",
    "text_fake = \"Nikita poshel v magazin v Amerike i uvidel druga iz Googla\"\n",
    "tag_text(text_fake, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "1. Model managed to zero-transfer LOC \"_Kali\"(DE) to \"_Cali\"(FR).\n",
    "2. zero-shot for non-existent language \"Ruskiy\" (Russian using Latin letters) also does good NER\n",
    "\n",
    "\n",
    "Let's evaluate the whole test dataset FR, IT and EN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_lang = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1(trainer, dataset=panx_lang[\"test\"])\n",
    "\n",
    "f1['de']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['de']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['de']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['de']['de']=}\") # 0.867\n",
    "print(f\"{f1['de']['fr']=}\") # 0.705\n",
    "print(f\"{f1['de']['it']=}\") # 0.675\n",
    "print(f\"{f1['de']['en']=}\") # 0.586\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% and 67% good FR/IT zero-shot transfer. The model has never seen a single labeled Frech or Italian example.\n",
    "\n",
    "58% is okay, but ENG shot from DE expected to be higher.\n",
    "\n",
    "Let's now analyze how good was DE zero-shot on FR:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine-tune FR\n",
    "\n",
    "Let's fine-tune base XLMRoBERTa on FR using different len of train dataset to compare with DE zero-shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset['train'].shuffle(seed=42).select(range(num_samples))\n",
    "    valid_ds = dataset['validation']\n",
    "    test_ds = dataset['test']\n",
    "\n",
    "    training_args.push_to_hub = False\n",
    "    training_args.logging_steps = len(train_ds)\n",
    "\n",
    "    trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                      data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                      train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "    return get_f1(trainer, test_ds)\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    f1s = {}\n",
    "    for ns in [250, 500, 1000, 2000, 4000]:\n",
    "        f1s[ns] = train_on_subset(dataset=panx_fr_encoded, num_samples=ns)\n",
    "else:\n",
    "    f1s = {\"num_samples\": [250, 500, 1000, 2000, 4000, 15000], \n",
    "          \"f1\": [0.179, 0.604, 0.733, 0.817, 0.838, 0.87]}\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(panx_de_encoded['train']), len(panx_fr_encoded['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(f1[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\n",
    "metrics_df.set_index(\"num_samples\").plot(ax=ax)\n",
    "plt.legend([\"12k DE Zero-shot\", \"(x)k FR Fine-tuned\"], loc=\"lower right\")\n",
    "plt.ylim((0, 1))\n",
    "plt.title(\"F1 score on PANX.fr[test] by models\")\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"F1 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Fine-tuning on Multiple Languages\n",
    "\n",
    "### First, let's concat DE and FR datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "panx_encoded['defr'] = DatasetDict()\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    panx_encoded['defr'][split] = concatenate_datasets([panx_de_encoded[split],  panx_fr_encoded[split]]).shuffle()\n",
    "\n",
    "\n",
    "training_args.logging_steps = len(panx_encoded['defr']['train']) // batch_size\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer, train_dataset=panx_encoded['defr'][\"train\"],\n",
    "    eval_dataset=panx_encoded['defr'][\"validation\"])\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    trainer.train()\n",
    "    trainer.save_model('defr')\n",
    "else:\n",
    "    trainer.model = (MyXLMRoBERTaForTokenClassification\n",
    "             .from_pretrained('../../docs/colab_training/defr')\n",
    "             .to(device))\n",
    "\n",
    "f1['defr']['de'] = evaluate_lang_performance('de', trainer)   \n",
    "f1['defr']['fr'] = evaluate_lang_performance('fr', trainer)\n",
    "f1['defr']['it'] = evaluate_lang_performance('it', trainer)\n",
    "f1['defr']['en'] = evaluate_lang_performance('en', trainer)\n",
    "\n",
    "print(f\"{f1['defr']['de']=}\") # 0.870\n",
    "print(f\"{f1['defr']['fr']=}\") # 0.887\n",
    "print(f\"{f1['defr']['it']=}\") # 0.814\n",
    "print(f\"{f1['defr']['en']=}\") # 0.677"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
