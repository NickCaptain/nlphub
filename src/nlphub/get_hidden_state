
import torch

def get_hidden_state(data_encoded, model, tokenizer):

    extract_hidden_state_model = lambda batch : get_hidden_state(batch, model=model, tokenizer=tokenizer)

    data_encoded.set_format("torch")
    data_hidden = data_encoded.map(extract_hidden_state, batched=True)
    return data_hidden

def extract_hidden_state(batch, model, tokenizer, device=None):

  if device == None:
    device = 'cpu'

  inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}

  with torch.no_grad():
    last_hidden_state = model(**inputs).last_hidden_state

  # return THE FIRST token aka [CLS]
  return {"hidden_state" : last_hidden_state[:,0].cpu().numpy()}